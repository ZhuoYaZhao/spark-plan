原文：https://calculatedcontent.com/2019/04/01/sf-bay-acm-talk-heavy-tailed-self-regularization-in-deep-neural-networks/,[md](HEAVY-TAILED-SELF-REGULARIZATION-IN-DEEP-NEURAL-NETWORKS.md)

作者： [**Charles H Martin, PhD**](https://calculatedcontent.com/author/charlesmartin14/) 

翻译：[muzhoubai](https://github.com/muzhoubai)

# SF BAY ACM TALK：深层神经网络中的大规模自调整

我的协作者在当地的旧金山湾ACMMeetup上就我们的研究发表了出色的工作。

Michael W. Mahoney加州大学伯克利分校

随机矩阵理论（RMT）用于分析深度神经网络（DNN）的权重矩阵，包括生产质量，预训练模型和从头开始训练的较小模型。实证和理论结果清楚地表明，DNN训练过程本身隐式地实现了一种自我调节的形式，隐式地雕刻了更加规范化的能量或惩罚态势。特别是，即使在没有外生指定显式正则化的传统形式的情况下，DNN层矩阵的经验谱密度（ESD）也会显示传统规则化统计模型的签名。基于RMT的相对较新的结果，最引人注目的是将其扩展到重尾矩阵的通用性类别，并将其应用于这些经验结果，我们开发了一种理论来确定5 + 1训练阶段，这对应于隐式自训练量的增加正则化。对于较小和/或较旧的DNN，这种隐式自正则化就像传统的Tikhonov正则化一样，因为似乎存在“大小尺度”，将信号与噪声分开。但是，对于最先进的DNN，我们确定了一种重尾自规则化的新形式，类似于无序系统的统计物理学中看到的自组织。这种隐式的自我调节可能在很大程度上取决于训练过程的许多过程。特别是，通过利用泛化差距现象，我们证明了可以通过更改批处理大小来使一个小模型展示训练的所有5 + 1阶段。这表明，在所有其他条件都相同的情况下，具有较大批处理量的DNN优化会导致隐式正则化模型的欠完善，并为泛化差距现象提供了解释。与Calculation Consulting，Inc.的Charles Martin共同合作。

简历：https://www.stat.berkeley.edu/~mmahoney/

Michael W. Mahoney在统计局的UCB和国际计算机科学研究所（ICSI）中。他从事现代大规模数据分析的算法和统计方面的工作。他最近的大部分研究都集中在大规模机器学习上，包括随机矩阵算法和随机数值线性代数，用于在大型信息图中提取结构的几何网络分析工具，可扩展的隐式正则化方法以及在遗传学，天文学，医学成像中的应用。 ，社交网络分析和互联网数据分析。他在耶鲁大学获得博士学位，主修计算统计力学。他曾在耶鲁大学数学系，雅虎研究部和斯坦福大学数学系任教。他是统计与应用数学科学研究所（SAMSI）的国家顾问委员会成员，还是国家研究委员会海量数据分析委员会成员。他联合组织了大数据分析理论基础的西蒙斯研究所（Simons Institute）2013年秋季计划，并主持了两年一次的MMDS现代海量数据集算法研讨会。他目前是UC Berkeley的NSF / TRIPODS资助的FODA（数据分析基金会）研究所的首席PI。他拥有在Yahoo Research和Vieu Labs，Inc.的首席数据科学家的多项专利，Vieu Labs，Inc.是一家为数十亿用户重新设计消费视频的初创公司。

有关更多信息，请访问https://www.stat.berkeley.edu/~mmahoney/

论文的长版（作为演讲的基础）：https://arxiv.org/abs/1810.01075http://www.meetup.com/SF-Bay-ACM/http://www.sfbayacm.org /