*作者：[Quan Chen](https://github.com/chenquan)*

>  在scikit-learn中，RandomForest的分类器是`RandomForestClassifier`，回归器是`RandomForestRegressor`，需要调参的参数包括两部分，第一部分是`Bagging`框架的参数，第二部分是`CART决策树`的参数。

### 一、Bagging框架的参数：

**1.** **n_estimators:** 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数，默认是10。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。对Random Forest来说，增加“子模型数”（n_estimators）可以明显降低整体模型的方差，且不会对子模型的偏差和方差有任何影响。模型的准确度会随着“子模型数”的增加而提高，由于减少的是整体模型方差公式的第二项，故准确度的提高有一个上限。在实际应用中，可以以10为单位，考察取值范围在1至201的调参情况。

**2.** **bootstrap**：默认True，是否有放回的采样。

**3.** **oob_score：**默认为False，即是否采用袋外样本来评估模型的好坏。有放回采样中大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)，这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。*个人推荐设置为True*，因为袋外分数反应了一个模型拟合后的泛化能力。对单个模型的参数训练，我们知道可以用cross validation（cv）来进行，但是特别消耗时间，而且对于随机森林这种情况也没有大的必要，所以就用这个数据对决策树模型进行验证，算是一个简单的交叉验证，性能消耗小，但是效果不错。

**4.** **criterion：** 即CART树做划分时对特征的评价标准，分类模型和回归模型的损失函数是不一样的。分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益entropy，是用来选择节点的最优特征和切分点的两个准则。回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae。*一般来说选择默认的标准就已经很好的。*

**从上面可以看出， RF重要的框架参数比较少，主要需要关注的是 n_estimators，即RF最大的决策树个数。**

 

### 二、决策树的参数：

**1.** **max_features:** RF划分时考虑的最大特征数。可以使用很多种类型的值，默认是"None",意味着划分时考虑所有的特征数；如果是"log2"意味着划分时最多考虑log2N个特征；如果是"sqrt"或者"auto"意味着划分时最多考虑N−−√N个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数，其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的"None"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。传统决策树模型在选择特征时考虑所有可能的特征，而它降低了单个树的多样性，而由于随机森林基于集成学习思想的优点，减小max_features不仅会提升算法速度，也有可能降低测试误差，这也是RF模型在Bagging集成学习方法基础上的一个改进；对max_features的选择是逐一尝试，直到找到比较理想的值。

**2.** **max_depth:** 决策树最大深度。默认为"None"，决策树在建立子树的时候不会限制子树的深度这样建树时，会使每一个叶节点只有一个类别，或是达到min_samples_split。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。

**3.** **min_samples_split:** 内部节点再划分所需最小样本数，默认2。这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2，如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

**4.** **min_samples_leaf:**叶子节点最少样本数。 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

**5.** **min_weight_fraction_leaf：**叶子节点最小的样本权重和。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。

**6.** **max_leaf_nodes:** 最大叶子节点数。通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。

**7. min_impurity_split:** 节点划分最小不纯度。这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点，即为叶子节点 。一般不推荐改动默认值1e-7。

**8. splitter:** 随机选择属性"random"还是选择不纯度最大"best"的属性，建议用默认 best。

**9. presort:**是否对数据进行预分类，以加快拟合中最佳分裂点的发现。默认False，适用于大数据集。小数据集使用True,可以加快训练。是否预排序,预排序可以加速查找最佳分裂点，对于稀疏数据不管用，Bool，auto：非稀疏数据则预排序，若稀疏数据则不预排序.

### 三、预测

**1.predict_proba(x)：**给出带有概率值的结果。每个点在所有label（类别）的概率和为1. 

**2.predict(x)：**直接给出预测结果。内部还是调用的predict_proba()，根据概率的结果看哪个类型的预测值最高就是哪个类型。 

**3.predict_log_proba(x)：**和predict_proba基本上一样，只是把结果给做了log(x)处理。

### 四、注意事项

1. RF算法确实会过拟合。

2. 当算法中加入更多的树时，随机林中的泛化误差方差将减小到零，然而，泛化的偏差并没有改变。

3. 为了避免在RF中过拟合，应调整算法的超参数(hyper-parameters)，例如，叶子节点中的样本数。

4.  能处理高维度数据，并且不用做特征选择。
5.  训练完后，能够给出那些feature比较重要。
6.  在噪音较大的分类或回归问题上会出现过拟合现象。
7.  对于不同级别属性的数据，级别划分较多的属性会对随机森林有较大影响，则RF在这种数据上产出的数值是不可信的。 
8.  根据训练数据分布及业务逻辑，通过调节预剪枝的参数使树更深、每个树节点包含的不同数据更少来增强小样本标签数据的表现，但这样训练出来的模型仍然可能比较粗糙，并且需要准备不同时间窗或不同范围的数据进行多次验证，防止过拟合。 

